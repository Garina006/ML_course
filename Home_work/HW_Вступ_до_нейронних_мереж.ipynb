{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F-bbmtS0Ewp",
        "outputId": "c5d79999-676c-4c22-f6b5-c30e73e7b07f"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d6c9df-74ee-4aba-ca43-6dca771a5849"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cc244054c50>"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = inputs.shape[1]"
      ],
      "metadata": {
        "id": "bhuTXZsd2zK2"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(1, n_features, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obe_t-wt5Au2",
        "outputId": "649198c3-c8b6-4e21-b9db-d0910a03ee04"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.66135, 0.26692, 0.06168]], requires_grad=True),\n",
              " tensor([0.62132], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(data, w, b):\n",
        "  preds = 1 / (1 + torch.exp(-(data @ w.t() + b)))\n",
        "  return preds"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_preds = model(inputs, w, b)\n",
        "targets, model_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0gJu7G_-R4j",
        "outputId": "0722a498-9a49-41cb-9850-6cde06ac43c5"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]]),\n",
              " tensor([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В передбаченнях ми отримали всі значення = 1. Це і є підозрілим. Це можу бути зумовлено тим, що ознаки у нас мають доволі великі числа (від 37 до 134), плюс ваги так само великі. В результаті ми отримуємо e в ступіні - велике число, що в свою чергу дає результат, що прагне до 0. Тому 1 / 1 + 0 = 1"
      ],
      "metadata": {
        "id": "DpNwAbcBAJcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "  p = predicted_probs.squeeze().to(torch.float32)\n",
        "  y = true_labels.squeeze().to(torch.float32)\n",
        "  loss = (-(y * torch.log(p) + (1 - y) * torch.log(1 - p))).mean()\n",
        "\n",
        "  return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(model_preds, targets)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBoG597RIQSA",
        "outputId": "81b58d0d-8c60-4fe9-a143-7b75a0a8ca06"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(nan, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я перероблювала декілька раз свої функції але все одно отримую loss = nan. І це ніби як логічно, бо у мене всі пробабілітіс вийшли 1 і в цьому виразі torch.log(1 - p) я отримую log(0) - яке неможливе. Але виходячи з наведеного нижче пояснення, після backpropagation я б мала отримати якість мізерні але значення, а я отримую nan. То ж я йду далі, бо так можна застрягнути на цьому навічно."
      ],
      "metadata": {
        "id": "4XVg0BBKU8Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsLAqOY4R1OE",
        "outputId": "cef4db87-4ea0-4616-8480-913b4d4c2ffb"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.66135, 0.26692, 0.06168]], requires_grad=True)\n",
            "tensor([[nan, nan, nan]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHFUqHbYUx84",
        "outputId": "323353f9-6c1c-4a91-ae4a-d8a600282c26"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.62132], requires_grad=True)\n",
            "tensor([nan])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_preds = model(inputs, w, b)\n",
        "targets, model_preds"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf9e990-49c9-4590-eeaa-3b7a9e3a13e9"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]]),\n",
              " tensor([[0.51735],\n",
              "         [0.52205],\n",
              "         [0.52436],\n",
              "         [0.52045],\n",
              "         [0.51904]], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(model_preds, targets)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2xz8Rf5W2MW",
        "outputId": "8fb6d8a3-9e30-47a4-f3bb-d7b040b17abc"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.68295, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "py9kprISXAWm"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0cb2db-732e-4217-9fa3-dde649b9b543",
        "id": "zzBiFpDAXjVt"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0.00066,     0.00027,     0.00006]], requires_grad=True)\n",
            "tensor([[ -5.44172, -18.98529, -10.06817]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18b7ab5-5a5c-488b-f98d-c8672ea92e31",
        "id": "N-0j9__hXjVv"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.00062], requires_grad=True)\n",
            "tensor([-0.07935])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "_uuid": "9f5f0ffeee666b30c5828636359f0be6addbef7c",
        "id": "_LaxHxYment1"
      },
      "outputs": [],
      "source": [
        "for i in range(1000):\n",
        "    preds = model(inputs, w, b)\n",
        "    loss = binary_cross_entropy(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "preds, targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kvb8bASaZhi",
        "outputId": "a10cf43c-83b5-4fb9-b687-bc27d31f70c2"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.57766],\n",
              "         [0.66854],\n",
              "         [0.91132],\n",
              "         [0.16164],\n",
              "         [0.86534]], grad_fn=<MulBackward0>),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(preds, targets)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ucZ-EXwao0V",
        "outputId": "76fe577b-04c2-42d1-eff3-e7701e2927b9"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.33568, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Виходячи з отриманого результату, модель працює непогано. Маємо впевнені пробабілітіс для всіх значень, окрім першого. Отже на першому значенні модель помилилась. Помилка зменшилась в 2 рази від початкового значення 0.6829."
      ],
      "metadata": {
        "id": "kW9LnypFbCr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-8ld95VdKrh",
        "outputId": "b52d288d-ddc7-4e4d-880a-7f87107ad49d"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset"
      ],
      "metadata": {
        "id": "297wVRVdeEHy"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = TensorDataset(inputs, targets)"
      ],
      "metadata": {
        "id": "9IV4ZX47dgr9"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uD__aVSc9s6",
        "outputId": "d83e34d5-f0af-4d6f-c685-69958e7e7284"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(data, batch_size, shuffle=True)\n",
        "sample = next(iter(train_dl))\n",
        "sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9OJ5vUUgOYN",
        "outputId": "1b5455be-5d88-4b58-da63-2b7376dabf73"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [102.,  43.,  37.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "Jd2Paw9Pjsgc"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = inputs.shape[1]\n",
        "b = targets.shape[1]\n",
        "a,b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMH8PyUfioCd",
        "outputId": "5497a1e0-dbc5-4253-daf4-5fe1da1c0290"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogReg(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(a, b)\n",
        "        self.act1 = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.act1(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogReg()"
      ],
      "metadata": {
        "id": "Dag7Vgn_j1R0"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
        "loss_lr = F.binary_cross_entropy"
      ],
      "metadata": {
        "id": "9nTKokbSkJP9"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(sample[0])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hur_2_WGlHv9"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(sci_mode=False, precision=5)\n",
        "preds.detach(), sample[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzC0-OInrihC",
        "outputId": "f2c689a9-159b-4db9-a425-d7756ccbb97c"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[    0.17603],\n",
              "         [    0.03367],\n",
              "         [    0.99996],\n",
              "         [    0.00004],\n",
              "         [    0.99996]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_lr(preds, sample[1])"
      ],
      "metadata": {
        "id": "DyXg5LsVouys"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JitYl9Ho3zE",
        "outputId": "dec42c8c-9d4b-4e61-9b08-35f3530c2d1d"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.81674, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На одному маленькому семплі модель геть невірно передбачила мітки."
      ],
      "metadata": {
        "id": "dQUmSdFTpcTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = fit_return_loss(1000, model, loss_lr, opt, train_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z38YMQogsQ--",
        "outputId": "d04e0bd3-5d32-4e6d-a6c2-7accbe6b365f"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 6.6400\n",
            "Epoch [20/1000], Loss: 6.0483\n",
            "Epoch [30/1000], Loss: 5.7263\n",
            "Epoch [40/1000], Loss: 5.4710\n",
            "Epoch [50/1000], Loss: 5.2474\n",
            "Epoch [60/1000], Loss: 5.0653\n",
            "Epoch [70/1000], Loss: 4.8974\n",
            "Epoch [80/1000], Loss: 4.7861\n",
            "Epoch [90/1000], Loss: 4.6903\n",
            "Epoch [100/1000], Loss: 4.5353\n",
            "Epoch [110/1000], Loss: 4.4753\n",
            "Epoch [120/1000], Loss: 4.3686\n",
            "Epoch [130/1000], Loss: 4.2792\n",
            "Epoch [140/1000], Loss: 4.2018\n",
            "Epoch [150/1000], Loss: 4.1132\n",
            "Epoch [160/1000], Loss: 4.0389\n",
            "Epoch [170/1000], Loss: 3.9423\n",
            "Epoch [180/1000], Loss: 3.8701\n",
            "Epoch [190/1000], Loss: 3.7756\n",
            "Epoch [200/1000], Loss: 3.6974\n",
            "Epoch [210/1000], Loss: 3.6101\n",
            "Epoch [220/1000], Loss: 3.5297\n",
            "Epoch [230/1000], Loss: 3.4427\n",
            "Epoch [240/1000], Loss: 3.3760\n",
            "Epoch [250/1000], Loss: 3.2822\n",
            "Epoch [260/1000], Loss: 3.1985\n",
            "Epoch [270/1000], Loss: 3.1111\n",
            "Epoch [280/1000], Loss: 3.0308\n",
            "Epoch [290/1000], Loss: 2.9483\n",
            "Epoch [300/1000], Loss: 2.8748\n",
            "Epoch [310/1000], Loss: 2.7879\n",
            "Epoch [320/1000], Loss: 2.7048\n",
            "Epoch [330/1000], Loss: 2.6292\n",
            "Epoch [340/1000], Loss: 2.5456\n",
            "Epoch [350/1000], Loss: 2.4710\n",
            "Epoch [360/1000], Loss: 2.4122\n",
            "Epoch [370/1000], Loss: 2.3102\n",
            "Epoch [380/1000], Loss: 2.2363\n",
            "Epoch [390/1000], Loss: 2.1587\n",
            "Epoch [400/1000], Loss: 2.0854\n",
            "Epoch [410/1000], Loss: 2.0062\n",
            "Epoch [420/1000], Loss: 1.9342\n",
            "Epoch [430/1000], Loss: 1.8632\n",
            "Epoch [440/1000], Loss: 1.7922\n",
            "Epoch [450/1000], Loss: 1.7232\n",
            "Epoch [460/1000], Loss: 1.6485\n",
            "Epoch [470/1000], Loss: 1.5853\n",
            "Epoch [480/1000], Loss: 1.5150\n",
            "Epoch [490/1000], Loss: 1.4520\n",
            "Epoch [500/1000], Loss: 1.3831\n",
            "Epoch [510/1000], Loss: 1.3171\n",
            "Epoch [520/1000], Loss: 1.2595\n",
            "Epoch [530/1000], Loss: 1.2084\n",
            "Epoch [540/1000], Loss: 1.1388\n",
            "Epoch [550/1000], Loss: 1.0851\n",
            "Epoch [560/1000], Loss: 1.0415\n",
            "Epoch [570/1000], Loss: 0.9843\n",
            "Epoch [580/1000], Loss: 0.9393\n",
            "Epoch [590/1000], Loss: 0.8865\n",
            "Epoch [600/1000], Loss: 0.8436\n",
            "Epoch [610/1000], Loss: 0.8030\n",
            "Epoch [620/1000], Loss: 0.7632\n",
            "Epoch [630/1000], Loss: 0.7308\n",
            "Epoch [640/1000], Loss: 0.7033\n",
            "Epoch [650/1000], Loss: 0.6679\n",
            "Epoch [660/1000], Loss: 0.6437\n",
            "Epoch [670/1000], Loss: 0.6134\n",
            "Epoch [680/1000], Loss: 0.5918\n",
            "Epoch [690/1000], Loss: 0.5703\n",
            "Epoch [700/1000], Loss: 0.5492\n",
            "Epoch [710/1000], Loss: 0.5325\n",
            "Epoch [720/1000], Loss: 0.5149\n",
            "Epoch [730/1000], Loss: 0.5012\n",
            "Epoch [740/1000], Loss: 0.4870\n",
            "Epoch [750/1000], Loss: 0.4743\n",
            "Epoch [760/1000], Loss: 0.4654\n",
            "Epoch [770/1000], Loss: 0.4552\n",
            "Epoch [780/1000], Loss: 0.4421\n",
            "Epoch [790/1000], Loss: 0.4315\n",
            "Epoch [800/1000], Loss: 0.4263\n",
            "Epoch [810/1000], Loss: 0.4149\n",
            "Epoch [820/1000], Loss: 0.4085\n",
            "Epoch [830/1000], Loss: 0.4013\n",
            "Epoch [840/1000], Loss: 0.3940\n",
            "Epoch [850/1000], Loss: 0.3883\n",
            "Epoch [860/1000], Loss: 0.3829\n",
            "Epoch [870/1000], Loss: 0.3770\n",
            "Epoch [880/1000], Loss: 0.3719\n",
            "Epoch [890/1000], Loss: 0.3672\n",
            "Epoch [900/1000], Loss: 0.3627\n",
            "Epoch [910/1000], Loss: 0.3587\n",
            "Epoch [920/1000], Loss: 0.3549\n",
            "Epoch [930/1000], Loss: 0.3522\n",
            "Epoch [940/1000], Loss: 0.3474\n",
            "Epoch [950/1000], Loss: 0.3445\n",
            "Epoch [960/1000], Loss: 0.3411\n",
            "Epoch [970/1000], Loss: 0.3395\n",
            "Epoch [980/1000], Loss: 0.3350\n",
            "Epoch [990/1000], Loss: 0.3319\n",
            "Epoch [1000/1000], Loss: 0.3297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "LEMCy0S5siZA",
        "outputId": "0580043f-794e-4038-e564-b7772c635767"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSfRlk8FsryQ",
        "outputId": "0aa90205-c397-4716-9903-63302437255c"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.57286],\n",
              "        [0.67915],\n",
              "        [0.90261],\n",
              "        [0.15694],\n",
              "        [0.87609],\n",
              "        [0.57286],\n",
              "        [0.67915],\n",
              "        [0.90261],\n",
              "        [0.15694],\n",
              "        [0.87609],\n",
              "        [0.57286],\n",
              "        [0.67915],\n",
              "        [0.90261],\n",
              "        [0.15694],\n",
              "        [0.87609]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_cls = (preds >= 0.5).to(torch.int64).squeeze(1)   # 0/1\n",
        "y_pred_cls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCWN47JruTRc",
        "outputId": "686c2fe5-a39b-4867-acf4-cc8d9354a908"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyrYmfIhtIcG",
        "outputId": "d3d23ecd-dc72-446d-9009-20e0d5c9dcbd"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель в цілому натренувалась непогано. Бачимо зменшення помилки активно відбувається до 1000 епохи. Є вирогідність, що зі збільшенням епох помилка все ще продовжувала би зменшуватись, хоч і не так активно.  \n",
        "Модель вірно визначила всі екземпляри класу 0, а от на класі 1 допустила 3 помилки. При цьому пробабілітіс в усіх 3 випадках складають 0.57286, тобто модель саме на цих екземплярах не дуже впевнена. Зменшивши поріг, можемо досягти 100% попадання по всіх екземплярах."
      ],
      "metadata": {
        "id": "dnliy4Kdu925"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2x8vJHjufEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}